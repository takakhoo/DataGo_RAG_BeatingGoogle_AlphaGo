# tuning/phase2/phase2_recursion.py
"""
Phase 2b: recursion control sweep (offline JSON-driven workflow)
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from statistics import mean
from typing import Any, Dict, List, Optional

from common.game_runner import GameRunner, GameMetrics, DEFAULT_SIMILARITY_WEIGHTS


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Phase 2b: recursion depth tuning")
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("./tuning_results/phase2/recursion"),
        help="Where to store per-config metrics and summary JSON.",
    )
    parser.add_argument(
        "--positions-json-dir",
        type=Path,
        required=True,
        help="Directory containing offline JSON logs of complex positions.",
    )
    parser.add_argument(
        "--phase1-config",
        type=Path,
        default=Path("./tuning_results/phase1/best_config_phase1.json"),
        help="Phase 1 learned weights JSON file.",
    )
    parser.add_argument(
        "--storage-config",
        type=Path,
        default=Path("./tuning_results/phase1b/storage_threshold_results.json"),
        help="Phase 1b storage threshold JSON file.",
    )
    parser.add_argument(
        "--deep-mcts-results",
        type=Path,
        default=Path("./tuning_results/phase2/deep_mcts/phase2_deep_mcts_results.json"),
        help="Summary JSON generated by phase2_deep_mcts.py.",
    )
    parser.add_argument(
        "--num-samples",
        type=int,
        default=20,
        help="Number of synthetic games per recursion depth value.",
    )
    parser.add_argument(
        "--positions-per-game",
        type=int,
        default=32,
        help="How many JSON entries to treat as one synthetic game.",
    )
    parser.add_argument(
        "--baseline-deep-visits",
        type=int,
        default=10_000,
        help="Visit count used when offline deep analyses were produced.",
    )
    parser.add_argument(
        "--shallow-visits",
        type=int,
        default=800,
        help="Visit count for shallow (baseline) search.",
    )
    parser.add_argument(
        "--target-move-time-ms",
        type=float,
        default=2_500.0,
        help="Reference move time used in scoring.",
    )
    parser.add_argument(
        "--lambda-time",
        type=float,
        default=0.1,
        help="Penalty weight for move time in recursion scoring.",
    )
    parser.add_argument(
        "--lambda-depth",
        type=float,
        default=0.2,
        help="Penalty weight for recursion depth usage in scoring.",
    )
    parser.add_argument(
        "--lambda-error",
        type=float,
        default=0.5,
        help="Penalty weight for residual value error in scoring.",
    )
    return parser.parse_args()


def load_json(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as fh:
        return json.load(fh)


def aggregate_metrics(metrics: List[GameMetrics]) -> Dict[str, float]:
    if not metrics:
        return {
            "avg_move_time_ms": 0.0,
            "avg_value_error": 0.0,
            "avg_policy_error": 0.0,
            "avg_deep_time_ms": 0.0,
            "avg_recursion_depth": 0.0,
            "deep_search_total": 0,
            "deep_converged_total": 0,
        }

    return {
        "avg_move_time_ms": mean(m.avg_move_time_ms for m in metrics),
        "avg_value_error": mean(m.avg_value_error for m in metrics),
        "avg_policy_error": mean(m.avg_policy_error for m in metrics),
        "avg_deep_time_ms": mean(m.avg_deep_time_ms for m in metrics),
        "avg_recursion_depth": mean(m.max_recursion_depth_used for m in metrics),
        "deep_search_total": sum(m.deep_search_count for m in metrics),
        "deep_converged_total": sum(m.deep_converged_count for m in metrics),
    }


def compute_score(
    metrics: Dict[str, float],
    *,
    target_move_time_ms: float,
    lambda_time: float,
    lambda_depth: float,
    lambda_error: float,
) -> float:
    time_ratio = (
        metrics["avg_move_time_ms"] / target_move_time_ms if target_move_time_ms else 0.0
    )
    return (
        -lambda_error * metrics["avg_value_error"]
        -lambda_time * time_ratio
        -lambda_depth * metrics["avg_recursion_depth"]
    )


def main() -> None:
    args = parse_args()
    args.output_dir.mkdir(parents=True, exist_ok=True)

    for required_path in [args.positions_json_dir, args.deep_mcts_results]:
        if not required_path.exists():
            raise FileNotFoundError(f"Required path not found: {required_path}")

    phase1_cfg = load_json(args.phase1_config)
    storage_cfg = load_json(args.storage_config)
    deep_summary = load_json(args.deep_mcts_results)
    best_deep_entry: Optional[Dict[str, Any]] = deep_summary.get("best")
    if not best_deep_entry:
        raise RuntimeError(
            "phase2_deep_mcts_results.json does not contain a 'best' entry."
        )
    best_config = best_deep_entry["config"]

    runner = GameRunner(
        mode="offline_json",
        phase1_config=phase1_cfg,
        storage_config=storage_cfg,
        similarity_weights=DEFAULT_SIMILARITY_WEIGHTS,
        positions_json_dir=args.positions_json_dir,
        positions_per_game=args.positions_per_game,
        shallow_visits=args.shallow_visits,
        baseline_deep_visits=args.baseline_deep_visits,
    )

    recursion_candidates = [1, 2, 3, 5]
    all_results: List[Dict[str, Any]] = []
    best_entry: Optional[Dict[str, Any]] = None
    best_score = float("-inf")

    for idx, recursion_depth in enumerate(recursion_candidates, start=1):
        print(f"[Phase2/Recursion] ({idx}/{len(recursion_candidates)}) N={recursion_depth}")

        samples: List[GameMetrics] = []
        for sample_idx in range(args.num_samples):
            gm = runner.run_game(
                deep_mcts_max_depth=best_config["deep_mcts_max_depth"],
                policy_delta=best_config["policy_delta"],
                value_delta=best_config["value_delta"],
                recursion_depth_N=recursion_depth,
                seed=sample_idx,
            )
            samples.append(gm)

        aggregated = aggregate_metrics(samples)
        score = compute_score(
            aggregated,
            target_move_time_ms=args.target_move_time_ms,
            lambda_time=args.lambda_time,
            lambda_depth=args.lambda_depth,
            lambda_error=args.lambda_error,
        )
        result_entry = {
            "recursion_depth_N": recursion_depth,
            "score": score,
            "metrics": aggregated,
        }
        all_results.append(result_entry)

        output_path = args.output_dir / f"N{recursion_depth}.json"
        with output_path.open("w", encoding="utf-8") as fh:
            json.dump(result_entry, fh, indent=2)

        if score > best_score:
            best_score = score
            best_entry = result_entry

    summary = {
        "results": all_results,
        "best": best_entry,
        "scoring": {
            "lambda_time": args.lambda_time,
            "lambda_depth": args.lambda_depth,
            "lambda_error": args.lambda_error,
            "target_move_time_ms": args.target_move_time_ms,
            "num_samples": args.num_samples,
        },
        "dependent_deep_config": best_config,
    }
    summary_path = args.output_dir / "phase2_recursion_results.json"
    with summary_path.open("w", encoding="utf-8") as fh:
        json.dump(summary, fh, indent=2)

    if best_entry:
        print(
            "[Phase2/Recursion] Best recursion depth:",
            best_entry["recursion_depth_N"],
            "score=",
            best_entry["score"],
        )
    else:
        print("[Phase2/Recursion] No valid recursion configs evaluated.")


if __name__ == "__main__":
    main()